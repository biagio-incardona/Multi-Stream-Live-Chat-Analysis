{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Stream-Live-Chat-Analysis\n",
    "\n",
    "###### Incardona Biagio\n",
    "\n",
    "## What is it?\n",
    "\n",
    "Twitch-Youtube-Chat-Analysis is a university project made by the student Incardona Biagio for the **TECHNOLOGIES FOR ADVANCED PROGRAMMING** class. The project consists of a real-time analysis of the viewers' interactions of a live stream on YouTube and/or Twitch.\n",
    "\n",
    "\n",
    "## For Who?\n",
    "\n",
    "The project is aimed to streamers especially those who streams on multiple platforms in parallel (in this case: Twitch and YouTube).\n",
    "\n",
    "The project can also be used by potential investors to identify whether the streamer has a good group of viewers or not\n",
    "\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "The main goal is to have a simply interface to analyze the viewers behavior as the chats in a live stream can be really confusing.\n",
    "That issue became worse if parallel streams in different platforms are made\n",
    "\n",
    "\n",
    "A dashboard will then be provided containing the main analysis with the possibility of:\n",
    "  1. Having a summary analysis concerning all the platforms simultaneously\n",
    "  2. Having an analysis aimed at the single platform\n",
    "\n",
    "## Structure\n",
    "\n",
    "The project can be split into 5 different phases:\n",
    "    \n",
    "   1. [Ingestion](#Ingestion)\n",
    "   2. [Streaming](#Streaming)\n",
    "   3. [Processing](#Processing)\n",
    "   4. [Indexing](#Indexing)\n",
    "   5. [Visualization](#Visualization)\n",
    "\n",
    "## Ingestion\n",
    "\n",
    "The ingestion phase consists in the collection of the data from external sources, in this case our sources will be YouTube and Twitch.\n",
    "\n",
    "In order to get an easily expandable project, it has been decided to collect data using APIs and web scraping, then these data have been ingested using **Logstash**\n",
    "\n",
    "\n",
    "### Twitch\n",
    "\n",
    "To retain data from Twitch, its API has been used.\n",
    "\n",
    "### YouTube\n",
    "\n",
    "Even if YouTube exposes good APIs, it doesn't allow to have access to live chat resources without having a GUI, since the project must work entirely in batch **selenium** has been used to scrape the data from YouTube.\n",
    "\n",
    "Google Chrome's driver has been used.  \n",
    "\n",
    "#### Let's see how\n",
    "\n",
    "```python\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    browser = webdriver.Chrome(chrome_options=chrome_options)\n",
    "    browser.get(\"https://www.youtube.com/live_chat?v=\" + channel)\n",
    "    chats = []\n",
    "    count = 0\n",
    "    rate = 0.0\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        for chat in browser.find_elements_by_css_selector('yt-live-chat-text-message-renderer'):\n",
    "            usr = chat.find_element_by_css_selector(\"#author-name\").get_attribute('innerHTML')\n",
    "            mex = chat.find_element_by_css_selector(\"#message\").get_attribute('innerHTML')\n",
    "            usr = str(usr).split('<')[0]\n",
    "\n",
    "            obj = json.dumps({'author_name': usr, 'message': mex})\n",
    "            x = json.loads(obj)\n",
    "```\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "One of our goals is to extract public engagement.\n",
    "\n",
    "For our purpose, we define engagement as the time elapsed between sending one message and another.\n",
    "\n",
    "Such a value will be calculated with the following function.\n",
    "\n",
    "```python\n",
    "rate = 6/(end_time-start_time)\n",
    "```\n",
    "\n",
    "### Logstash\n",
    "\n",
    "Logstash is a tool for collecting, processing, and forwarding log events and messages. The collection is done through configurable input plug-ins. Once an input plug-in has collected the data, it can be processed by any number of filters that modify and annotate the event data. Finally Logstash routes events to output plugins which can forward events to a variety of external programs including Elasticsearch, local files, Kafka, and others.\n",
    "\n",
    "For our purpose, Logstash will collect the data from the two scrapers in JSON format over a TCP connection and then forward it to Kafka using the \"data\" topic.\n",
    "\n",
    "To allow the passage of data between Logstash and the two scrapers, a small protocol is needed in order to send homogeneous data.\n",
    "\n",
    "```\n",
    "{\n",
    "    'username' : username of the sender,\n",
    "    'message' : body of the message,\n",
    "    'engagement' : engagement,\n",
    "    'timestamp' : timestamp of when the message arrives into logstash,\n",
    "    'source' : source name\n",
    "}\n",
    "```\n",
    "\n",
    "## Streaming\n",
    "\n",
    "To carry out the data streaming Apache Kafka has been used.\n",
    "\n",
    "### Apache Kafka\n",
    "\n",
    "Apache Kafka is a distributed data streaming platform that allows you to publish, subscribe, archive, and process streams of records in real-time. It is designed to manage data streams from multiple sources by distributing them to multiple consumers. In short, it allows you to move large amounts of data from one point to another at the same time.\n",
    "\n",
    "## Processing\n",
    "\n",
    "Once we have collected the data through Kafka we want to manipulate and obtain additional information from this data.\n",
    "\n",
    "For this purpose, I relied on Apache Spark.\n",
    "\n",
    "### Apache Spark\n",
    "\n",
    "Apache Spark is a distributed computing framework that runs 100 times faster than its main competitor (MapReduce).\n",
    "\n",
    "Spark is optimized for machine learning algorithms, so it is obvious that my choice falls on him.\n",
    "\n",
    "For the creation and manipulation of DataFrame on the fly we made use of PySpark, the API provided by Spark to interface with him via Python.\n",
    "\n",
    "\n",
    "### Machine Learning\n",
    "By relying on Spark we can apply ML algorithms very quickly.\n",
    "\n",
    "Our goals are 2:\n",
    "    \n",
    "     1. Sentiment analysis of chats\n",
    "     2. Keyword extraction of chats\n",
    "    \n",
    "#### 1. Sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7351\n",
      "0.7351\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    value = sid.polarity_scores(text)\n",
    "    value = value['compound']\n",
    "    return value\n",
    "\n",
    "print(get_sentiment(\"i'm ugly :(\"))\n",
    "print(get_sentiment(\"i'm pretty :)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Keyword extraction\n",
    "\n",
    "To carry out the extraction of the keywords in each message I used one of the most powerful libraries of multi-language natural language processing currently existing for Python: **SpaCy**.\n",
    "\n",
    "Also providing 3 versions of the library: small (11MB), medium (~ 100MB), and large (~ 1GB), it can also be adapted to any memory-related problems.\n",
    "\n",
    "I obviously used the \"large\" version.\n",
    "\n",
    "##### Code\n",
    "```python\n",
    "def get_keyword(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN']\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "\n",
    "    back = [x for x in Counter(result).most_common(1)]       \n",
    "    if len(back) == 0:\n",
    "        return None \n",
    "    return back[0][0]\n",
    "```\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "At the end of the day, a JSON file structured in the following way will be provided\n",
    "\n",
    "```\n",
    "{\n",
    "    'username' : username dell utente,\n",
    "    'timestamp' : timestamp di quando un messaggio e arrivato,\n",
    "    'mex' : contenuto del messaggio,\n",
    "    'engagement' : engagement,\n",
    "    'source' : nome della sorgente,\n",
    "    'mex_sentiment' : valore della sentiment analysis del messaggio,\n",
    "    'keyword' : keyword rappresentativa del messaggio\n",
    "}\n",
    "```\n",
    "\n",
    "## Indexing\n",
    "\n",
    "Our data will be indexed in order to reduce research costs. For this purpose, we will make use of **ElasticSearch**.\n",
    "\n",
    "### ElasticSearch\n",
    "\n",
    "ElasticSearch is a search engine based on Lucene, the fastest Open Source data retrieval API used for the development of search engines.\n",
    "\n",
    "All the functionalities are natively exposed through the RESTful interface, while the information is managed as JSON documents.\n",
    "\n",
    "In the project, we will use only one index: \"data\"\n",
    "\n",
    "The use of a single index allows us to switch from one platform to another or work together in a very simple and intuitive way\n",
    "\n",
    "## Visualization\n",
    "\n",
    "Kibana is an open-source data visualization dashboard for Elasticsearch.\n",
    "sending the functionality of viewing indexed content on Elasticsearch. Users can create bar, line, and pie charts and maps over large volumes of data.\n",
    "\n",
    "In addition to running graphs, Kibana allows the processing of data in real-time!\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./dashboard.png\" width=\"1000\" title=\"hover text\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
