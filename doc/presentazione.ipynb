{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TwitchYoutubeChatAnalysis\n",
    "\n",
    "###### Incardona Biagio X81000608\n",
    "\n",
    "## Cos'è?\n",
    "\n",
    "Twitch-Youtube-Chat-Analysis è un progetto universitario realizzato per il corso **TECHNOLOGIES FOR ADVANCED PROGRAMMING** dallo studente Incardona Biagio, matricola **X81000608**.\n",
    "Il progetto consiste nell'analisi in real time delle interazioni degli spettatori di un live stream su YouTube e/o Twitch.\n",
    "\n",
    "Il progetto è stato realizzato in modo da essere facilmente estendibile a nuove piattaforme.\n",
    "\n",
    "### #MaiUnaGioia\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./progetto_meme.jpg\" width=\"400\" title=\"hover text\">\n",
    "</p>\n",
    "\n",
    "## Per chi?\n",
    "\n",
    "Il progetto è mirato a tutti gli streamer, specialmente per coloro che abitualmente effettuano stream su più piattaforme in parallelo, in questo caso YouTube e Twitch.\n",
    "\n",
    "Tale progetto può anche essere utilizzato da potenziali investitori (brand in cerca di pubblicità) al fine da evitare di investire su profili con views comprate o con un basso impatto sul pubblico.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./Perche_usarlo.jpg\" width=\"400\" title=\"hover text\">\n",
    "</p>\n",
    "\n",
    "\n",
    "## Obbiettivo\n",
    "\n",
    "Lo scopo ultimo di tale progetto è quello di avere un'interfaccia semplificata per analizzare il comportamento degli spettatori, in quanto le chat in una live stream sono spesso molto confusionare.\n",
    "Tale problematica viene ulteriormente amplificata se si effettuano gli stream in parallelo su più piattaforme.\n",
    "\n",
    "Verrà quindi fornita una dashboard contenente le analisi principali con la possibilità:\n",
    "  1. Avere un'analisi riassuntiva riguardante entrambe le piattaforme contemporaneamente\n",
    "  2. Avere un'analisi mirata alla singola piattaforma\n",
    "\n",
    "## Struttura\n",
    "\n",
    "il progetto può essere suddiviso in 5 fasi separate:\n",
    "    \n",
    "   1. [Ingestion](#Ingestion)\n",
    "   2. [Streaming](#Streaming)\n",
    "   3. [Processing](#Processing)\n",
    "   4. [Indexing](#Indexing)\n",
    "   5. [Visualization](#Visualization)\n",
    "    \n",
    "<p align=\"center\">\n",
    "  <img src=\"./Schema.png\" width=\"600\" title=\"hover text\">\n",
    "</p>\n",
    "\n",
    "## Ingestion\n",
    "\n",
    "La fase di ingestion dei dati consiste nella raccolta degli stessi da fonti esterne, in questo caso le nostre fonti saranno YouTube e Twitch.\n",
    "\n",
    "Al fine di ottenere una semplice estendibilità del progetto si è scelto di raccogliere questi dati tramite le API o tramite web scraping ed in seguito incanalare tutti i dati verso un'unica piattaforma : **Logstash**.\n",
    "\n",
    "\n",
    "### Twitch\n",
    "\n",
    "Per raccogliere i dati di Twitch si sono utilizzate le API fornite dallo stesso.\n",
    "\n",
    "Si effettuano richieste in loop ed i dati ricevuti vengono inseriti in un json ed in fine inviati a Logstash.\n",
    "\n",
    "Al fine di ottenere una consistenza temporale senza l'interferenza della latenza delle varie tecnologie utilizzate si è deciso di inserire in questa fase il timestamp di quando un dato è arrivato\n",
    "\n",
    "### YouTube\n",
    "\n",
    "Nonostante YouTube fornisca delle ottime API, questo è troppo rispettoso della privacy per i nostri gusti, non permette infatti di accedere alle risorse di una live chat ammenoché non sia la nostra e comunque sotto esplicito consenso dell'utente fornito tramite interfaccia grafica. Ma questo blocco ha fatto scattare la nostra carta trappola: **Selenium**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./selenium_meme.jpg\" width=\"600\" title=\"hover text\">\n",
    "</p>\n",
    "\n",
    "#### Selenium\n",
    "\n",
    "Selenium è il più diffuso tool per l’automazione di browser in commercio. Nonostante la sua principale funzione non sia quella di web testing automatico, esso viene utilizzato principalmente per questo.\n",
    "\n",
    "Tale prodotto con le opportune impostazioni può diventare un ottimo Web Scraper per siti web dinamici, impossibili da analizzare con un classico web scraper.\n",
    "\n",
    "Selenium ci permette non solo di analizzare il sorgente di una pagina web, ma anche il codice creato dopo l'esecuzione degli script, in questo caso in JavaScript.\n",
    "\n",
    "Per far ciò si è scelto di usare i driver di Google Chrome in modo da emulare esattamente il comportamento del browser web.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./meme_google.jpg\" width=\"600\" title=\"hover text\">\n",
    "</p>\n",
    "\n",
    "#### Vediamo come\n",
    "\n",
    "```python\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    browser = webdriver.Chrome(chrome_options=chrome_options)\n",
    "    browser.get(\"https://www.youtube.com/live_chat?v=\" + channel)\n",
    "    chats = []\n",
    "    count = 0\n",
    "    rate = 0.0\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        for chat in browser.find_elements_by_css_selector('yt-live-chat-text-message-renderer'):\n",
    "            usr = chat.find_element_by_css_selector(\"#author-name\").get_attribute('innerHTML')\n",
    "            mex = chat.find_element_by_css_selector(\"#message\").get_attribute('innerHTML')\n",
    "            usr = str(usr).split('<')[0]\n",
    "\n",
    "            obj = json.dumps({'author_name': usr, 'message': mex})\n",
    "            x = json.loads(obj)\n",
    "```\n",
    "\n",
    "### Un po di preprocessing\n",
    "\n",
    "Uno dei nostri obbiettivi è quello di estrapolare l'engagement sul pubblico.\n",
    "\n",
    "Per il nostro scopo definiamo engagement come il tempo trascorso tra l'invio di un messaggio e un altro.\n",
    "\n",
    "Tale valore verrà calcolato con la seguente funzione\n",
    "\n",
    "```python\n",
    "rate = 6/(end_time-start_time)\n",
    "```\n",
    "\n",
    "### Logstash\n",
    "\n",
    "Logstash è uno strumento per raccogliere, elaborare e inoltrare eventi e messaggi di registro. La raccolta viene eseguita tramite plug-in di input configurabili. Una volta che un plug-in di input ha raccolto i dati, può essere elaborato da qualsiasi numero di filtri che modificano e annotano i dati dell'evento. Infine Logstash indirizza gli eventi ai plugin di output che possono inoltrare gli eventi a una varietà di programmi esterni tra cui Elasticsearch, file locali, Kafka e altri.\n",
    "\n",
    "Per il nostro scopo Logstash raccoglierà i dati dai due scraper in formato json tramite una connessione TCP e li inverà in seguito a Kafka usando il topic \"data\".\n",
    "\n",
    "Per permettere il passaggio dei dati tra Logstash e i due scraper serve un piccolo protocollo in modo da inviare dati omogenei.\n",
    "\n",
    "```\n",
    "{\n",
    "    'username' : username dell utente,\n",
    "    'message' : contenuto del messaggio,\n",
    "    'engagement' : engagement ,\n",
    "    'timestamp' : timestamp di quando un messaggio e arrivato,\n",
    "    'source' : nome della sorgente\n",
    "}\n",
    "```\n",
    "\n",
    "## Streaming\n",
    "\n",
    "Per effettuare il data streaming ci si è appoggiati ad Apache Kafka.\n",
    "\n",
    "### Apache Kafka\n",
    "\n",
    "Apache Kafka è una piattaforma per il data streaming distribuita che permette di pubblicare, sottoscrivere, archiviare ed elaborare flussi di record in tempo reale. È progettata per gestire flussi di dati provenienti da più fonti distribuendoli a più consumatori. In breve, consente di spostare grandi quantità di dati da un punto qualsiasi a un altro nello stesso momento.\n",
    "\n",
    "## Processing\n",
    "\n",
    "Una volta raccolti i dati tramite kafka vogliamo manipolare e ottenere informazioni aggiuntive da questi dati.\n",
    "\n",
    "A tal fine ci si è appoggiati a Apache Spark.\n",
    "\n",
    "### Apache Spark\n",
    "\n",
    "Apache Spark è un framework per il calcolo distribuito che risulta 100 volte più veloce rispetto al suo principale concorrente (MapReduce).\n",
    "\n",
    "Spark è ottimizzato per algoritmi di machine learning, risulta ovvio quindi che la nostra scelta ricade su di lui.\n",
    "\n",
    "Per la creazione e la manipolazione di DataFrame on the fly si è fatto uso di PySpark, delle API fornite da Spark per interfacciarsi a lui tramite Python.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./pyspak_errore.jpg\" width=\"600\" title=\"hover text\">\n",
    "</p>\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "Appoggiandoci a Spark possiamo applicare algoritmi di ML in maniera molto rapida.\n",
    "\n",
    "I nostri obbiettivi sono 2:\n",
    "    \n",
    "    1. Sentiment analysis delle chat\n",
    "    2. Keyword extraction delle chat\n",
    "    \n",
    "#### 1. Sentiment analysis\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./scelta_vader.jpg\" width=\"600\" title=\"hover text\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7351\n",
      "0.7351\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    value = sid.polarity_scores(text)\n",
    "    value = value['compound']\n",
    "    return value\n",
    "\n",
    "print(get_sentiment(\"i'm ugly :(\"))\n",
    "print(get_sentiment(\"i'm pretty :)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Keyword extraction\n",
    "\n",
    "Per effettuare l'estrazione delle keyword in ogni messaggio si è fatto uso di una delle più potenti librerie di multi language natural language processing attualmente esistenti per Python : **SpaCy**.\n",
    "\n",
    "Fornendo inoltre 3 versioni della libreria : small(11MB), medium(~100MB) e large(~1GB) permette di essere adattata anche ad eventuali problematiche legate alla memoria.\n",
    "\n",
    "Noi ovviamente abbiamo utilizzato la versione large.\n",
    "\n",
    "##### Code\n",
    "```python\n",
    "def get_keyword(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN']\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "\n",
    "    back = [x for x in Counter(result).most_common(1)]       \n",
    "    if len(back) == 0:\n",
    "        return None \n",
    "    return back[0][0]\n",
    "```\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Alla fine di tutto il verrà fornito un file JSON strutturato nel seguente modo\n",
    "\n",
    "```\n",
    "{\n",
    "    'username' : username dell utente,\n",
    "    'timestamp' : timestamp di quando un messaggio e arrivato,\n",
    "    'mex' : contenuto del messaggio,\n",
    "    'engagement' : engagement,\n",
    "    'source' : nome della sorgente,\n",
    "    'mex_sentiment' : valore della sentiment analysis del messaggio,\n",
    "    'keyword' : keyword rappresentativa del messaggio\n",
    "}\n",
    "```\n",
    "\n",
    "## Indexing\n",
    "\n",
    "Indicizzeremo i nostri dati in modo da abbattere i costi per la ricerca. A tal fine faremo uso di **ElasticSearch**.\n",
    "\n",
    "### ElasticSearch\n",
    "\n",
    "ElasticSearch è un motore di ricerca basato su Lucene, le API di reperimento dati Open Source più veloce e utilizzato per la realizzazione dei motori di ricerca.\n",
    "\n",
    "Tutte le funzionalità sono nativamente esposte tramite interfaccia RESTful, mentre le informazioni sono gestite come documenti JSON.\n",
    "\n",
    "Nel progetto sfrutteremo un solo indice : \"data\"\n",
    "\n",
    "L'utilizzo di un solo indice ci permette di passare da una piattaforma all'altra o lavorare insieme in maniera molto semplice ed intuitiva\n",
    "\n",
    "## Visualization\n",
    "\n",
    "Kibana è una dashboard di visualizzazione dati open source per Elasticsearch.\n",
    "Fornisce funzionalità di visualizzazione dei contenuti indicizzati su Elasticsearch. Gli utenti possono creare grafici a barre, a linee e a torta e mappe su grandi volumi di dati.\n",
    "\n",
    "Oltre ad effettuare grafici, kibana permette l'elaborazione di dati in tempo reale!\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./dashboard.png\" width=\"1000\" title=\"hover text\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "# Idee per il futuro\n",
    "\n",
    "* inserire nuove piattaforme di streaming\n",
    "\n",
    "# GRAZIE PER L'ATTENZIONE\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./saluti_meme.jpg\" width=\"600\" title=\"hover text\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
